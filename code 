# backend/api.py
from fastapi import FastAPI, WebSocket
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
 
MODEL_NAME = "mistralai/Mistral-7B-Instruct"  # example
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")
chat = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)
 
app = FastAPI()
 
class Query(BaseModel):
    prompt: str
 
@app.post("/chat")
async def chat_endpoint(q: Query):
    out = chat(q.prompt, max_new_tokens=256, do_sample=True, temperature=0.8)
    return {"response": out[0]["generated_text"]}
 
@app.websocket("/ws/chat")
async def ws_chat(ws: WebSocket):
    await ws.accept()
    while True:
        data = await ws.receive_text()
        out = chat(data, max_new_tokens=256)[0]["generated_text"]
        await ws.send_text(out)
 
6) Local dev UI prototype (Streamlit)
pip install streamlit requests
 
# frontend/simple_ui.py
import streamlit as st
import requests
 
st.title("Traillofpossibilities — Prototype")
prompt = st.text_input("Say something to Traillofpossibilities")
if st.button("Send"):
    r = requests.post("http://localhost:8000/chat", json={"prompt": prompt})
    st.write(r.json()["response"])
 
 